
\documentclass[onecollarge,runningheads]{svjour2}
% 4 July 2017
\smartqed  % flush right qed marks, e.g. at end of proof
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{multirow}

\journalname{Journal of Biological Physics}
%\newcommand{\nexists}{\not\exists}
\begin{document}
\title{Extending network-based modeling in biological systems with algebraic topology
}
\subtitle{A solution for the network threshold selection problem\\ }
%\titlerunning{Short form of title}        % if too long for running head
\author{Jaime Gomez-Ramirez        \and
        Jose Luis Perez-Velazquez %etc.
}
%\authorrunning{Short form of author list} % if too long for running head
\institute{The Hospital for Sick Children \at
			  Department of Neuroscience and Mental Health \\
              Bay, 686 Toronto, Canada   \\
              \email{jaime.gomez-ramirez@sickkids.ca}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           %\and
           %S. Author \at
           %   second address
}
%\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor.
\maketitle

\begin{abstract}
The pace at which data is being produced poses important challenges at either conceptual and computational level. The high dimensionality, heterogeneity and complexity of biological data has deeply transformed the way in which life scientists work. Network theory, machine learning and stochastic modeling are now standard de facto in systems biology and neuroscience.
We highlighted two fundamental limitations in the graphical representation via graphs of multivariate data. First, the dyadic or pairwise representation of coactivation is unfit to encode higher order interactions and second, the derivation of the adjacency matrix from the correlation/covariance matrix relies upon the selection of a significance level or threshold which is necessarily ad-hoc and biased.
Algebraic topology provides a multi-scale mathematical framework that effectively deal with those limitations. In particular, the \emph{simplicial complex}, a high dimensional structure that generalizes network and \emph{persistent homology}, a method for computing topological features at different dimensional scales or space resolutions are introduced and applied to model fMRI connectivity data.

\keywords{metabolic network systems biology \and Category Theory  \and Graph Theory \and Functor\and emergent properties\and Graph Theory\and Algebraic topology \and Persistent homology \and Causality}
\end{abstract}

\section{Introduction}
% The ontological primacy of relations
Following the steps of Poincar{\'e}, modern physics teaches us that what science can attain is not the knowledge of things themselves but only the relations between the things \cite{Poincare:1952}. The physical world is a network of interacting systems with reciprocal information and a satisfactory understanding of how this interaction evolves is the primordial goal of science \cite{rovelli2015relative}.
This coupling can be precisely defined in mathematical terms as a correlation or distance between two measurements.

%Staement of the problem
A graph is a set of nodes or vertices, some of them connected forming edges or links. Graphs are built by connecting two elements, that is to say, the only type of relationship possible is between pairs. The limitation of having only dyadic (or bivariate in statistics jargon) relationships have important limitations that are often overlooked \cite{giusti2016two}.
Binary or unweighted networks are the result of thresholding or selecting a significance level and this is problematic, particularly when the underlying system is not scale invariant. For example, small world or clusterness are joint measures that can change very drastically depending on the choice of the threshold \cite{toppi2012statistical}. Furthermore, by adopting a threshold, we may be loosing important information, for example, it may occur that some small-scale features are noise artifacts while other are critically important \cite{fallani2014graph}, \cite{papo2014complex}.

Algebraic topology \cite{munkres1984elements} provides a language and a methodology to deal with the limitations in the graph theoretic approach, namely, the dyadic or pairwise nature of relationships and the threshold selection problem. 
The origins of this approach can be traced back to the seminal work of catastrophe theorist Christopher Zeeman on the topological structure of visual perception \cite{zeeman:1962}. While there has been since then and even before an understanding that differential equations are not 'good enough' to model a system of the staggering complexity of biological organisms \cite{rosen1958representation}, \cite{Anderson:1972}, \cite{wolkenhauer2001systems}, \cite{boogerd_all_2007}, it is only thanks to the wide availability and ever increasing power of modern computers that geometrical approaches such as persistent homology, have been successfully used to model the brain. Differential equations capture local invariants while algebraic topology, on the other hand, is adapted to capture global properties \cite{hoffman1966lie}. 
%https://elifesciences.org/content/3/e03476   %PUT REFS Ojo esto viene casi tal cual del papaer de Giusti
%https://www.lms.ac.uk/content/topological-theory-brain zeeman
Despite the very recent use of computational and algebraic topology tools in the life sciences important insights have been produced, including among others, the geometric structure of neural correlations \cite{giusti2015clique}, \cite{dotko2016topological}, the dynamics in gene regulatory networks \cite{boczko2005structure}, viral evolution and spread \cite{chan2013topology}, \cite{taylor2015topological}, genetic markers for breast cancer \cite{nicolau2011topology}, the analysis of protein structure \cite{xia2014persistent}, protein binding \cite{kovacev2016using} and protein classification \cite{cang2015topological}.
Persistence diagrams \cite{edelsbrunner2000topological}, a formalism to describe how topology changes with the
across changes in scales, have been used to quantify branching and looping of brain artery trees at multiple scales \cite{bendich2016persistent}, characterize cortical thickness data \cite{chung2009persistence}, detection of white matter alteration in brain morphometry \cite{chung2015persistent}, multimodal imaging \cite{lee2017integrated}, compare brain activity networks in resting state functional magnetic resonance imaging \cite{petri2014homological}, \cite{cassidy2015brain}, \cite{lord2016insights}, population activity in the visual cortex \cite{singh2008topological}, \cite{pirino2015topological}, understanding the neural code \cite{curto2017makes}, \cite{curto2017can}, in particular, the neural representation of spatial topology in rodents \cite{chen2014neural} and to reconceptualize the hippocampal map as a topological template specialized in computing a topological representation of the environment \cite{dabaghian2012topological}, \cite{dabaghian2014reconceiving}, \cite{babichev2016topological}.
Very recently, algebraic topology has proved to be effective in modeling complex biological networks as exemplified in the reconstruction and simulation of neocortical microcircuitry, comprising 8 million connections between 31,000 neurons \cite{markram2015reconstruction}. This is the first algebraic topological analysis of structural connectomics and connectomics-based spatio-temporal activity in a biologically realistic neural microcircuit.  In \cite{reimann2017cliques}, Henry Markram team of researchers using digital reconstruction show that neurons bound to functional cliques could play a major role in the emergence of correlated activity. In this view, the brain processes stimuli by building increasingly complex topological objects.

The main goal of this paper is to introduce the language of algebraic topology and explain what has to offer to mathematical biologists, biophysicists or anybody interested in modeling spatio-temporal patterns in biological systems.
We provide a gentle introduction to simplicial complexes -a generalization of graphs able to capture polyadic or multiple coactivity patterns- and persistent  homology -a methodology able to model persistence against changes of scale.
The rest of the paper is organized as follows. Section \ref{se:at} provides an introduction to algebraic topology, defining simplicial complexes and persistent homology.
%and the rips filtration and functorality
%we introduce topological data analysis and its most important tool, persistent homology.
In Section \ref{se:tda} we justify the need of a multiscale framework by exposing the inherent difficulties of the network paradigm to deal with the threshold selection problem and to encoding multiple coactivity patterns. 
An application of persistent homology using resting state fMRI brain connectivy data is provided in Section \ref{se:exa}. We conclude with the Conclusions and Discussion section.
%on the future of algebraic topology in biological systems modeling and its implications in a causal theory of biological function.

%Statistical shape theory, Darcy Thompson, finite configuration of points in euclidean space. Euclidean space imposes a choice in the form of Euclidean distance from the axioms or the corpus, but is not necessarily the only possible distance, for example, in genomics one may want ot use Hamming distance (between sequences of memeber of an alphabet). But all this is not useful for large set of data and Gromov-Hausdorff distance is very difficult to compute, so one is often looking for some kind of qualitative features. Note that metrics are useful but sometimes reflect an intuitive notion of similarity, close are similar, far apart are different. Distances are quite arbitrary. Geometry can be defined as the study of metrics and topology studies properties of geometric objects that remain after one allows oneself to stretch and deform the object without tearing it.
%A ball has B0 =1 (one connected component), B1 = 0 (no holes), B2 = 1, torus is 1,2,1 a double torus
%The homology that persists (the cycles) across epsilon is what we are looking for

\section{Algebraic topology}  % for biologists
\label{se:at}

In this section we will provide some definitions and examples to make some key concepts clear to the reader, specially for those not familiar with the field of topology. In particular, we provide definitions of algebraic topology, topological space, simplicial complex, filtration and persistent homology. The idea is to provide a smooth entry point to these techniques. For a more in detail treatment, see \cite{munkres1984elements}, \cite{hatcher2001} and for an introduction to topological data analysis see \cite{carlsson2009topology} and \cite{wasserman2016topological}. The reader familiar with the theory can directly skip to section \ref{se:tda}.

We will proceed providing the due definitions in this order: \emph{algebraic topology}, \emph{topology}, \emph{topological space}, \emph{simplicial complex} and its constituent parts called \emph{simplices} to finally study the \emph{persistent homology} method. 
%Simplical complex and persistent homology will be studied in Section \ref{se:tda} by examples.

\begin{definition}
Algebraic topology is the study of topological spaces using abstract algebra.
\end{definition}

There are two main approaches in the study of algebraic topology: \emph{i} decompose the topological space into many but simple pieces and \emph{ii} deal with only a few but complicated pieces. Computational topologists favour the first approach -many simple pieces- and the simplicial complex is the prime example of this. The second approach -few but complicated pieces- is of more interest for those looking for aesthetically pleasing shapes \cite{edelsbrunner2010computational}.

The goal of algebraic topology is to classify topological spaces up to homeomorphism, that is, find the intrinsic qualitative aspects of spatial objects that remain invariant under homeomorphic transformations. Definitions of homeomorphism, continuous maps and related terms can be find in the Appendix. Algebraic Topology is concerned with properties of geometric objects that are invariant under continuous deformations i.e. bending, twisting and stretching, but not tearing.
For example, a torus and a coffee mug are topologically equivalent both have one hole and it is always possible to get a coffee mug from a torus and viceversa via continuous transformations. On the other hand, a torus and a double torus (a sphere with two handles and two holes) are not homeomorphic because they have different number of holes.
It follows that the presence of holes in a geometric figure is the key property studied in algebraic topology, that is, the invariant under continuous deformation. 
Algebraic topology formalizes the notion i-dimensional hole or cavity. For example, given a 3-dimensional geometric object, finding a 0-dimensional hole entails that the geometric object has one connected component. A 1-dimensional hole is a cavity that one can see through e.g. the hole in a doughnut or a cheese hole. A 2-dimensional hole is a hollow space, e.g. a balloon one can't see through but there is air trap inside the surface (Figure \ref{fig:iholes})

% to draw figures use TikZ
% sphere, https://tex.stackexchange.com/questions/42812/3d-bodies-in-tikz
% double torus https://tex.stackexchange.com/questions/231515/draw-a-smooth-surfaced
% balloon https://tex.stackexchange.com/questions/134117/balloon-shape-in-tikz
 \begin{figure}[h]
%Figure 1, a is a ball, b is a torus c is a ballon
        \centering
        \includegraphics[width=0.1\linewidth]{../figures/800px-Double_torus_wiki}
        \caption{The figure shows the number of i-dimensional holes (i=0,1,2) for a compact sphere, a torus, a double torus and a balloon.
        Let $B_i$ ne the number of i-dimensional holes. For (a), $B_0=1, B_1=0, B_2=0$, a compact sphere has one connected component and no holes in it; (b) $B_0=1, B_1=1, B_2=0$, a torus has one connected component, one hole where one can see through and no hollow spaces; (c) $B_0=0, B_1=2, B_2=0$, a double torus has one connected component, two holes where one can see through and no hollow spaces and (d) $B_0=1, B_1=0, B_2=1$; a balloon has one connected component, no holes where one can see through and one hollow spaces.
        }
\label{fig:iholes}
\end{figure}

Since the goal of algebraic topology is to classify topological spaces. A formal definition of topology with examples is given in the appendix. It is worth reminding that sets are collection of points lacking structure, in order to study how the objects (data points) in a set are related, for example, who is connected with who, we need to find continuity maps. The goal of defining topologies in a set is to add structure, in particular, continuity.
A topological space can be understood as a set with the structure provided by the chosen topology. Formally,  

\begin{definition}
%Topological space
A topological space, T=(M,O), is an ordered pair or tuple consisting in a set M (for example, under the ZFC axioms \cite{suppes1960axiomatic}) and a topology O. %The topology O is a collection of subsets of M, satisfying the axioms in Definition \ref{def:topology}.
 \end{definition}

Topological spaces offer continuous models of reality and are the most general mathematical space, for instance, metric spaces are specializations of topological spaces equipped with extra structure or constraints.
%if we are only interested in connectedness, topological spaces are, in practical terms, similar to graphs.
For example, the set of points in the plane $M \subseteq \mathbb{R}^2$ together with the topology generated by the Euclidean metric is a topological space. To construct the topology, we call the set of points at distance less than $r, r > 0$ from a point $x \subseteq \mathbb{R}^2$ an open disk. Taking finite intersections and arbitrary unions of open disks, we get a collection of open sets that satisfies the conditions of topological space.
%A neighborhood is an open disk centered in the point.

%\subsection{Simplicial complex}
%\label{se:sc}
%Simplicial complex is an object from algebraic topology that deals satisfactorily with this problem.
%The boundary of a simplex consists of all possible subsets of its constituent vertices, called its faces.
%A simplicial complex encodes polyadic relations through its simplices.
%Because any given simplex is required to “contain all of its faces”, it suffices to specify only the maximal simplices, those which do not appear as faces of another simplex (Fig. 4c). This dramatically reduces the amount of data necessary to specify a simplicial complex, which helps make both conceptual work and computations feasible.
%YS: ojo literal

Once we have defined topological space we can define simplicial complex. A simplicial complex is a decomposition of a topological space into (topologically) simple pieces and their common intersections are lower-dimensional pieces of the same kind. Since simplicial complex generalizes graphs it is possibly easier to grasp the concept of simpicial complex by means of analogy with graph. 
A graph consists of a set of vertices and a set of edges forming a list of pair of vertices. A simplicial complex, on the other hand, is a set of vertices and a collection of simplices where a simplex is a finite set of vertices with the additional property that any subset of a simplex is also a simplex. In the same way as one can represent a graph as a collection of points and line segments (edges) between them, one can represent the simplices in a simplicial complex as a collection of solid regions spanning vertices. Formally,

\begin{definition}
An (abstract) simplicial complex X is a pair of sets: $V_X$ of vertices and $S_X$ of simplices, $X=(V_X,S_X)$. Each simplex in the set of simplices $S_X$ is a subset of the vertices $V_X$ plus the condition that if $\sigma$ is in $S_X$ then every subset $\tau$, $\tau \in \sigma$ is also in $S_X$, $\tau \in S_X$.
\end{definition}

A simplex with n elements (vertices) is called a $(n-1)-$simplex or what is the same the convex hull of n vertices (Convex hull is defined in the Appendix section). Simplices of small dimensions have proper names: a vertice is a 0-simplex, an edge is a 1-simplex, a triangle or face is a 2-simplex and a tetrahedron is a 3-simplex \footnote{Tetrahedron is the Platonic solid $P_5$ with four polyhedron vertices, six polyhedron edges, and four equivalent equilateral triangular faces}.

%construction of simplicial complex
Figure \ref{fig:vrcomplex} shows the construction of the Vietoris-Rips simplicial complex \cite{hausmann1995vietoris} which is the less demanding, computationally speaking, simplicial complex. The Vietoris-Rips complex is a two step process, first we connect the vertices (or data points) and second we build a n-complex for each group of n+1 points. The first step consists in connecting the points that are close to each other according to a threshold ($\epsilon$), drawing an edge (1-simplex) for every two data points whose distance is within the threshold. For example, two data points $u,v$ are connected if the euclidean distance (or any other similarity metric) is within the threshold $\epsilon$ , $d(u,v) < \epsilon$. At each step in the process we always want to add the higher dimensional simplex possible. Thus, in the next step we need to find 2-simplices, for that we need to build triangles by filling the space defined by three edges (1-dimensional simplices) in order to get a "side" or a 2-simplex. We can continue finding 3-dimensional simplices or tetrahedron whose boundary is the four faces given by 2-simplices and so on for higher dimensions that nevertheless, can not be easily visualized.

The idea behind the construction of the simplicial complexes depicted in Figure \ref{fig:vrcomplex} is that one can form a n-1 simplex with n points that are closed together. By the same token, a relationship between two vertices or two 0-simplex forms a 1-simplex (Figure \ref{fig:vrcomplex}(b)), a triangle or 2-simplex consists in putting together three edges or 1-simplex (Figure \ref{fig:vrcomplex}(c)), a tetrahedron or 3-simplex ifs defined by four 2-simplices or triangles (Figure \ref{fig:vrcomplex}(d)), similarly for 4-simplex and so on.
%Simplicial complexes encode diverse neural data modalities. After choosing the complex of interest, quantitative and theoretical tools can be used to describe, compare, and explain the statistical properties of their structure in a manner analogous to graph statistics or network diagnostics. (Giusti)

\begin{figure}[h]
%Figure 2 construction of Vietoris-Rips simplicial,for each graph (only vertices) and a fixed threshold 1,2,3 and 4 vertices
%Isabel K. Darcy simplicial complex from data https://www.youtube.com/watch?v=E7d7LHb6PLc&t=902s
% n different graphs provide n diffrent scs
        \centering
        \includegraphics[width=0.1\linewidth]{../figures/800px-Double_torus_wiki}
        \caption{The figure shows the simplicial complex obtained from different graphs and a fixed threshold, $\epsilon$ such that two vertices, $u, v$, are connected if the distance between u and v is less than the threshold, $d(u,v) < \epsilon$. Figure \ref{fig:vrcomplex}(a) shows a 0-dimensional simplicial complex from one vertice. (b) shows 1-dimensional simplicial complex obtained by connecting two vertices with an edge.  (c) depicts a 3 vertices networks all connected forming a 2-dimensional simplicial complex (triangle). (d) depicts a 4 vertices all connected forming a 3-dimensional simplicial complex (tetrahedron) the four 2-dimensional complexes are "faces" of the tetrahedron.}
\label{fig:vrcomplex}
\end{figure}

% Persistent Homology
As it was already mentioned, the simplicial complex generalizes network models and therefore network properties can be directly generalized. For example, the clustering coefficient of a node in a network can be translated into the number of simplices of different sizes in which the node participates, and the path length can be reformulated considering paths through simplices of fixed size \cite{dotko2016topological}. However, extending network metrics into simplicial complexes does not reckon the potential of simplicial complexes to encode data.

The study of the homology of simplicial complexes provides a way to identify structures that are overseen in network theory, specifically cycles, holes and flairs. Persistent homology a recent methodology from algebraic topology, only 20 years old, allows us to investigate qualitative features i.e. holes, of geometric objects. Prior to define persistent homology we need to understand \emph{filtration}, the process in which persistent homology relies upon.

Figure \ref{fig:vrcomplex} shows that  is a key factor for the obtention of the simplicial complex is the choice of the threshold $\epsilon$, for the same data set, two different threshold may generate 
different but also the same simplicial complexes. For example, two vertices separated a certain distance could be connected or not that is, form a 1-simplex depending on the threshold of choice.
Thus, depending on the election of the threshold we may get very different structures which is particularly problematic since the threshold is often selected on an ad hoc basis. 

An alternative to overcome this difficulty is to select a large population of threshold values. This process is called filtration. The filtration of a complex is the method by which the complex is reduced into its constituent pieces by consecutively applying different thresholds in order to build simplicial complexes whose simplices represent data points that are closed enough given the actual threshold. Formally,
\begin{definition}
Given a simplicial complex K, a filtration is a succession of growing subcomplexes on K or $K_0 \subseteq K_1 ...   \subseteq K_n$ where each subcomplex $K_i$ is the i-th level of the filtration.
\end{definition}
%A filtration is a 1-parameter family of geometric objects, $F(X) = \{U(X, \epsilon)\}, \epsilon = \{ 0, \infty\}$, that is, a union of balls with varying radius. Holes are then identified in an ensemble of objects (as the radius increases) rather than for an individual object.
%Nota 5.1. Una filtración se puede interpretar como una manera de obtener el complejo
%total K a partir de una familia de subcomplejos atendiendo a un determinado criterio,
%correspondiendo cada nivel Ki al estado de dicha construcción en el instante de tiempo i

% (for some choice of $\epsilon$).
% min 25 see the example
The intuitive idea behind filtration is to choose a real positive number, $\epsilon > 0$, ($\epsilon$ is a thickening parameter) and let $U(X,\epsilon)$ denote the union of balls of radius $\epsilon$ centered at the points of X and then calculate the Betti number $B_i(U(X,\epsilon))$ of this object (See the Appendix for a definition of Betti numbers). For each $i \geq 0$, we can define a barcode $B_i(X)$, which is a set of closed intervals in $\mathcal{R}$. For example, for a good choice of $\epsilon$ in the data set $X \subseteq \mathcal{R}^2$, the Betti number 0, $B_0$ detects the connected component in X, and the Betti number 1, $B_1$ detects the cycles in X.

Thresholding (or selecting significance level) is problematic, particularly when the underlying system has a combination of small-scale features, some of which are noise artifacts, and some of which are critically important. Figure \ref{fig:whyweneedfiltration} shows an example of a filtration of simplicial complex obtained by changing the $\epsilon$. It shows how to build the Vietoris Rips simplicial complex from a point cloud data set or set of vertices, a cycle or hole is identified in \ref{fig:whyweneedfiltration}(a), in \ref{fig:whyweneedfiltration}(b) using a different significance level teh hole is missed.
Figure \ref{fig:whyweneedfiltration}(a) identifies a cycle in a 2-dimensional image, this is not particularly challenging to see in a low dimensional space like the plane, but what if the data set seats in a let us say, 40-dimensional space?
The invariant that we are looking for may be unstable with respect to perturbation of data (noise) or small changes in $\epsilon$. For example, the thickening parameter or threshold of choice $\epsilon$ may not be able to distinguish large from small holes, also the invariant could be very sensitive to outliers. \ref{fig:whyweneedfiltration}(b) is an illustration of this point.

\begin{figure}[h]
%Figure 3 Why we need to do filtration, fixed epsilon fails to detect features (small holes or noisy data)---Hole detection Example
%MAke the point that fixed epsilon fails to detect features
%Lesnick IAS: Studying the shape of data using topoloy. Studying the Shape of Data Using Topology _ Institute for Advanced Study
%https://www.youtube.com/watch?v=XOZN3XZdoO0 (Chad Giusti)
        \centering
        \includegraphics[width=0.1\linewidth]{../figures/800px-Double_torus_wiki}
        \captioni{. Figure shows the filtration process obtained by changing the threshold $\epsilon$.
        The chosen $\epsilon$ in a) allows us to detect cycle or hole in the data set but it fails to detect it in b) . We start with 0 dimensional data points and we define closeness between points using a ball having the data point as the center, the threshold is the diameter of the ball and when two balls intersects the two data points are connected. From one unique data cloud when applied different thresholds monotonically increasing we obtain different simplical complexes. By increasing the threshold eventually all the data points will be part of the same simplex, forming one single component.}
\label{fig:whyweneedfiltration}
\end{figure}

%Filtration: Multiscale multidensity%
Crucially, filtrations reveal additional structure in the complex and provide tools for understanding how that structure arises and can be seen as an alternative to thresholding, providing a principled approach to binarizing graphs. Performing filtration to generate simplicial complexes would be incomplete without the analysis of the features of the data that emerge across the filtration process as a result of varying the closeness parameter ($\epsilon$). Furthermore, it is also the case that the complexes generated via filtration are related to one another and sophisticated measurements of structure i.e. homology, can be used to extract much finer detail of the evolution of the complexes as the threshold varies.
The idea of persistent homology is to look for features for instance a hole that persist for some range of parameter values. Betti numbers are the most typical feature studied in persistent homology. Typically, features, for example a hole, will appear and disappear as the parameter changes in value. 

%Filtrations can be used to assess the dynamics of neural processes \cite{kim2014morphological}, \cite{giusti2016two}.
%This graph filtration approach along with the persistent homology framework can be used to differentiate between classes of networks, so it can be a sort of hierarchical classifier. For example, used in differentiating between glucose metabolic network using 102 ROI from FDG-PET from ADHD, ASD and pediatric controls.
% Here biblio on filtration applied to biology

% https://ncatlab.org/nlab/show/persistent+homology
\begin{definition}
Persistent homology is a computational homology theory useful in analyzing large data sets. 
It identifies a ‘filtration structure’ of the complex by means of keeping track of the homology classes that stay ‘persistent’ upon changes in the data space resolution.
\end{definition}


%Thus, the Betti numbers of the complex X will vary with the parameter p, $B_p(X)$. For example, for p=0 , $B_0(X)$ identifies the number of connected components and $B_1(X)$ the number of holes etc.

Persistent homology was created in 2000 to detect the intrinsic structure of a manifold underlying point cloud data and provides robust computational methods for studying qualitative features. It produces simple descriptors of qualitative features of data called barcodes which are nothing but closed intervals in a line $\mathcal{R}$). Given a topological space, its homology is a formal, algebraic way to talk about its connectivity.
Persistent homology is computationally very heavy and it has only until very recently, thanks to dramatic improvements in processing capabilities of modern computers, that persistent homology methods have been put to use to solve real world problems.
However, homology in complex biological systems is still challenging. The computation of the permanent homology from correlation data of 100 neurons leads to a simplicial complex with approximately 107 4-simplices, the same computation for a population of 200 neurons (2x) involves two orders of magnitude more (100x).
To mitigate this combinatorial growth in complexity, different solutions exist, for a review see \cite{otter2015roadmap}

%reduction algorithms can be used as a sort of preprocessing to collapse the size of the complex.
%Another approach is to use the fact that homology can be computed locally and then aggregated, allowing for distributed computation over multiple processors and memory cores (Bauer et al. 2014). Finally, computing approximate homology further reduces complexity in difficult cases while still providing useful statistical information (Sheehy 2013).
%For a review in the state of the art of computational methods in topological analysis (persistent homology) .

%Consideremos la formación de un complejo simplicial usando bolas de diámetro ε cen- tradas en puntos de un conjunto. Para un ε pequeño, el complejo simplicial es sólo un conjunto disjunto de vértices. Pero para ε suficientemente grande, el complejo simplicial se convierte en un gran racimo (cluster) de símplices. Pero ¿cuál es el valor de ε que nos da la estructura “correcta”? La homología persistente nos da una respuesta rigurosa a esta pregunta.

In table \ref{table:table} we have grouped the concepts defined in this section.
%YS corrige one column lost https://www.overleaf.com/10856723dyjtdxxssxdr#/40815121/
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|l|}
\hline
 Concept & Definition\\ \hline
\multirow{3}{*}{Algebraic topology}
 %& {} \\
 & {The study of topological spaces using abstract algebra.}  \\
 %& {} \\
 \hline
\multirow{3}{*}{Topological space}
 %& { } \\
 & {T=(M,O) is an ordered pair or tuple consisting in a set M and a topology O.}  \\
 \hline
 \multirow{3}{*}{Topology}
 & {For a given set M, a topology O is a subset of the power set of M such that} \\
 & {i. The empty set and the set M are in the topology O }  \\
 & {ii. For any two subsets of O the intersection also exists in the topology O } \\
 & {iii. The union of any of subset of O also belongs to O} \\
 \hline
\multirow{3}{*}{Simplicial Complex}
 & {A SC is a pair os sets: $V_X$ of vertices and $S_X$ of simplices. Each simplex in the set of simplices $S_X$ }  \\
 & {is a subset of the vertices $V_X$ plus the condition that if $\sigma$ is in $S_X$ then every subset $\tau \in \sigma$ }  \\
 & the spais also in $S_X$, $\tau \in \S_X$.\\
 \hline
  \multirow{3}{*}{Filtration}
 & {1-parameter family of geometric objects, $F(X) = \{U(X, \epsilon)\}, \epsilon = \{ 0, \infty\}$, that is, }  \\
 & {a union of balls with varying radius.}  \\
 \hline\multirow{3}{*}{Persistent Homology}
 & {A method used in topological data analysis (TDA) to study qualitative features of data }  \\
 & {that persist across multiple scales. It is robust to perturbations of input data and  independent}  \\
 & {of dimensions and coordinates.} \\
 \hline
\end{tabular}
\label{table:table}
\end{center}
\end{table}

\section{Topological data analysis}
\label{se:tda}

Topological data analysis (TDA) is a relatively new branch of statistics (15 years old). TDA provides a theoretical framework to extract data features taking into account the multiscale and multidensity of complex data sets. In particular, persistent homology which was discussed in the previous section, allows data analysis with robustness to noise, dimensionality reduction and multiple scales.

The goal of TDA is to apply topology to study qualitative features of data. Of course, data means very different things depending on the context, in TDA data refers to any of this 2 things: \emph{i} a data cloud, i.e. a finite set of points in $\mathcal{R}^n$ with some notion of distance embedded and \emph{ii} functions, in particular functions that take values in the Real line, $f:\mathcal{X} \to \mathcal{R}, where $\mathcal{X}$ is any space and $\mathcal{R}$ is the space of real numbers. The first approach refers to unsupervised learning and the second deals with homeomorphic network objects (isomorphic continuous function between topological spaces). We briefly sketch the key notions of the first approach -qualitative features of finite set of points in $\mathcal{R}^n$ without any underlying structure- to delve more deeply into the second approach -qualitative analysis of simplices generated in the filtration process from network objects- which is the main focus of this paper.

When dealing with cloud data sets, the goal is partitioning the data into data structures such as clusters, cycles or i-dimensional holes. TDA provides formal definitions of those features, the computational tools for detecting such features and very importantly a methodology for quantifying the statistical significance of data features when the data is noisy (see Figure \ref{fig:whyweneedfiltration} for an example). The key premise of this method and of any unsupervised learning method is that of studying the shape of scientific data will likely give us insight into the processes that generated those data.
%ojo literal cambia esto, https://www.oreilly.com/ideas/data-has-a-shape(Data has shape, and shape has a meaning)
% It is possible to extend this analysis from a topological perspective, without relying on the network as a geometric object.
%YS Connection between topology and ML

Importantly, machine learning algorithms produce functional mappings from an input space to an output space without assuming a shape beforehand and to combine various machine learning techniques while maintaining guarantees about the underlying shape of the data. In standard machine learning, the shape of the data is usually an afterthought. Topology, on the other hand, puts the shape front and center -i.e., as being the most important aspect of your data.
Applications of TDA applied to machine learning (unsupervised classification) are multiple, including, the biophysics of proteins \cite{gameiro2015topological} genomics and virus evolution \cite{chan2013topology}, prognosis \cite{schmidt2011disease}, visual computing \cite{bendich2010computing}, coverage detection in wireless sensor networks \cite{ghrist2005coverage}, signal analysis \cite{perea2015sliding} etc.
%, shape segmentation, shape comparison and even basketball analytics (revealed 13 new basketball positions)
%https://www.wired.com/2012/04/analytics-basketball/
%Data has shape, and shape has meaning.


%Topology studies the mapping of one space into another through continuous deformations.
%Qualitative features refers to coarse-scale, global geometric features. Examples of qualitative features of point cloud data are Clusters, Cycles, Tendrils/Flares, Graph structure.  Our interest is in partitioning the data into data structures and very specially in the presence of low density noise. We can also find the analogue of these features in functions, Modes (peaks of the function) is the analogue of clusters, Craters is the analogue of cycles in the data (R2 to R, a volcano)...

In what follows we will focus on the topological approach, that is, qualitative analysis of the relations between the simplices generated in the filtration process from dataset defined as network objects. % , including a notion of distance.
Persistent homology provides the methodology for the qualitative analysis of the relations between the simplices generated in the filtration process. The motivation for the use of persistent homology in studying the simplicial complexes generated via filtration process of networks comes from the realization of two important limitations in the geometric approach based on network analysisn, namely the dyadic relationship and the threshold selection problem, which are described next.

\subsection{Limitations of the network approach}
\label{se:limita}
%This is in contrast with the algebraic topological approach which as we will show is better equipped to model complex biological networks.

\subsubsection{Dyadic or pairwise relationships}
\label{sse:dy}
For the sake of clarity and without loss of generality we frame the problem using statistical terminology.
%For a given data set each data point represents a collection of properties, for example, spatial coordinates, weight, age, temperature, pressure, time series etc.
Data points are related to each other based on a distance measure which can be defined quite liberally depending on the needs of the problem at hand. For example, the "closeness" of two data points can be expressed, among others, as a metric distance, a correlation coefficient or a coherency coefficient when the data points represent time series.
The closeness is always built upon the properties of the data points and requires a threshold. Thus, two points are close to each other when the distance among them is within a certain threshold. The choice of the threshold makes factual the relationship between data points. Two or more points are close to each other when the distance is within a certain threshold \cite{giusti2016two}.

The use of networks is predicated on a simplifying assumption: that the quintessential unit of interest is the dyad, two nodes (eg. neurons or brain regions) connected by an edge. While rarely mentioned, this assumption inherently limits the modelling scope of graphs. Simplicial complexes (algebraic topology) is a generalization of graphs that overcomes this limitation.

% YS formula of coherency What to do?
%One could argue that this is not exactly true if we define the distance using coherency.

%Coherency will identify the phase-lag and will distinguish between the pacemarker model (time-lag) and the simultaneous (zero-lag). %YS
%Coherency analysis is capable of characterizing coactivity, but we still have a problem when we want to express the existing relationships in a network. 

Because a network can only describe dyadic relationships between population elements, any binary coactivity network constructed from such observations would necessarily be identical. It does not matter which distance is used, correlation, mutual information, coherency etc. at the end the network formalism will force us to build dyadic relationships.

The crux of the matter is to have available a mathematical formalism able to capture higher order interactions. There are however, more versatile versatile forms able to explicitly encoding multiple (other than double, triple etc.) coactivity pattern exist. For instance, concurrence topology, a new nonparametric method for describing high-order dependence has been used to study the frequency of observation of coactivity patterns in fMRI recordings \cite{ellis2014describing}. Ellis and colleagues \cite{ellis2014describing} show that brain regions which are coactive change dynamically over time and that these cohesive functional units will appear more often than those that happen coincidental. This invalidates the method of selcting an a priori threshold to study the significance of the deepndencies among regions, for an illustration of this see Figure \ref{ex:dyadic}.
% we dont solve here the threshold, selection significance problem, it is in reality a problem of scale?
%Real geometry is that that persist against changes of scale to understand the shape we need to dynamically change the threshold.
%https://www.youtube.com/watch?v=kctyag2Xi8o

\begin{figure}[h]
  %Figure 4 pacemaker coactivity (model generated not fMRI)
        \centering
        \includegraphics[width=0.1\linewidth]{../figures/800px-Double_torus_wiki}
        \caption{The figure shows the inadequacy of graph to model high-order dependence among pairs of variables. A graph is a system of dyadic relationships and, very importantly, is unable to encode higher order interactions. In Figure \ref{ex:dyadic} a) theree nodes are connected though edges (1-simplex) denoting that the tree nodes are "connected". In Figure \ref{ex:dyadic} b), the nodes are connected via edges and there is also a face (triangle) which represents coactivity of the three brain regions or nodes. Thus, a) has three 1-simplex or edges that represent dependence among nodes (0-siplex) and b) has one face (2-simplex) which represent coactivity of the three brain regions or nodes. Simplicial complexes allows us to distinguish between having, for example, three connected nodes working as a pacemaker to having nodes tha are simultaneously active, importantly, there is nothing in the graph-theoretic formalism that will tell the difference.
         }
\label{ex:dyadic}
\end{figure}

\subsubsection{Threshold selection problem}
\label{sse:thr}
%Network comparison with persistent homology
%3.2.b. relationship between networks as the threshold changes. Filtration is the process of selecting all possible threshold and PH is the method to compute the topological in the resulting topological space (n dimensions))

The standard approach in network analysis consists in deriving a network from the covariance/variance matrix by establishing a threshold of convenience. For example, the threshold can be be equal to the mean plus one standard deviation of the distribution in the covariance/variance matrix. But it is not only possible but also preferible to study the relationship between networks as the threshold changes, that is, rather than imposing one threshold ad-hoc, we can study the network evolution as the threshold changes. 

Persistent homology allows to study the architecture as it evolves with every new threshold. For a binary network $B$, we obtain the sequence of networks $\{B_{\epsilon}\}$, where $\epsilon$ is a vector containing all the possible thresholds. In particular, for the Rips complex as $\epsilon$ increases the Rips complex becomes larger, that is, $B_{\epsilon_0} \in B_{\epsilon_1} ... B_{\epsilon_n}$, for $\epsilon_0 \leq \epsilon_1 ...\epsilon_n$. As the filtration value $\epsilon$ changes the topological properties of the Rips complex changes with it.
The connected components are merged during Rips filtration and can be represented with a dendogram, a tree diagram that is commonly used in clustering analysis.
Once the sequence of networks is built, we are left to study their evolution, for that we need to compute their distances. We can compare the networks with graph theoretic measures like assortativity, betweenness centrality etc. However, topological space generalize metric space this makes possible to transcend the standard network analysis.
Persistent homology can overcome the problem of thresholding (or selecting significance level) when the underlying system has a combination of small-scale features, some of which are noise artifacts, and some of which are critically important. In the next section we will show an application of persistence homology applied to brain connectivity data. 


\section{Persistence Homology applied to brain connectivity data}
\label{se:exa}

Persistent homology allows to study the architecture as it evolves with every new threshold. The idea behind 
persistent homology is to identify the features of data that persist across multiples scales as revealed in a filtration process. A graph filtration is in essence the outcome of the continuous variation of the threshold $\epsilon$ starting from a fully disconnected graph. Thus, for a given threshold $\epsilon$ two nodes a and b are linked if $d(a, b) \leq \epsilon$. When $\epsilon = 0$ the network is totally disconnected and has N (the number of nodes) components. The number of connected components increases as $\epsilon$ increases until the threshold is $\epsilon_{max}$ in which case the network is fully connected. 

In the following example we study the sequence of complexes in the filtration in order to compute the Betti numbers of the of the simplicial complexes that appear and disappear in the filtration.
%can be transformed via homology into an inter-related family of evolving cycles.
We calculate the simplicial complexes for the metric space $B_\epsilon$ of networks. Remember that as
we previously stated in Section \ref{se:at}, every metric space is a topological space.
For p regions of interest there are $2^p$ subsets candiates to form a topology. The power set as we saw in definition \ref{def:topology} is a representation of the underlying topology.
Given a (measurement) set $X$ and a rule of connection, the topological space is a simplicial complex and its elements are simplices. The important point is that a binary network is a simplicial complex consisting of 0-simplices (nodes) and 1-simplices (edges). A simplicial complex on the other hand, admits larger dimensions that can be identified via persistent homology methods such as Betti numbers. 

%A Rips complex is a kind of simplicial complex.
Given the set $X$, the Rips complex $R(X,\epsilon)$ is a simplicial complex whose k-simplices correspond to $k+1$-tuples of points that are pairwise within distance $\epsilon$.
While the binary networks have at most 1-semplices, the Rips complex has up to p-1-simplices, so the Rips complex can have faces as well. The Rips complex is the main algebraic representation of the entire brain and the multiscale networks are the Rips filtration which is a sequence of nested Rips complexes over different scales. The Betti numbers e.g. connected components, holes and voids, capture the topological information.
% Trivially, when we use the same distance to build the metric, we have $B(X,\epsilon) \in R(X,\epsilon)$.

We provide an example of the application of persistence homology applied to resting state functional magnetic resonance imaging data (rsFMRI). First, we have a measurement set denoted as $X= \{X_1 ... X_p\}$ where $x_i$ refers to the time series associated with a region of interest (ROI) in the resting state fMRI data set. The preprocessing specifics of the image are described in the Appendix section. 
The distance between two time series $X_i$ and $X_j$ extracted at regions $i$ and $j$ can be, for example, calculated using the Pearson correlation, $C_{X}(x_i ,x_j)= 1 - corr(x_i,x_j)$ or other form of coupling in time or frequency domain e.g. phase coupling, Granger causality, trasfer entropy etc.
% also Granger causality and other
%YS cor should be in abs value
The measurement set $X$ and the distance $C_X$ form the metric space $(X,C_X)$. The network constructed by thresholding the correlation matrix between nodes is trivially constructed as follows: if $C(X_i,X_j) \leq \epsilon$ there is a link between two nodes, otherwise the nodes are not connected.

Figure \ref{fig:ph1} shows the filtration process and the resulting Betti numbers. Figure \ref{fig:ph1} (d), shows the  \textit{barcode} which is the visualization of the changes in the Betti number over the Rips filtration. The barcode is thus, the topological invariance representation of the network change over filtration, so it lacks geometric information about the nodes, in order to have that we can incorporate the node information to the barcode to obtain a dendogram. Then we can compare the distance between network by calculating the Gromov distance between dendograms.
%Figure \ref{fig:ph1} (a) shows the pipeline -the correlation/covariance matrix we perform a filtration process with a vector $\epsilon$ of possible thresholds.
%Figure \ref{fig:ph1} (b) displays the resulting networks of applying each of the possible thresholds contained in the threshold vector $\epsilon= \epsilon_0,... \epsilon_i, ..., \epsilon_n$. Figure \ref{fig:ph1} (c) shows the Gramov-Hausdorff distance between the networks.
%In Figure \ref{fig:ph1} (d) the x axis represents the threshold  and the y axis contains the number of connected components in the other axis (barcode). The barcode in the two conditions show difference/not.
\begin{figure}[h]
%Figure 5. from fMRI
        \centering
        \includegraphics[width=0.1\linewidth]{../figures/800px-Double_torus_wiki}
        \caption{Binarize networks for different epsilon and calculate the GH distance or network cabnonical metrics and the barcodes or betti number for the threshold as it varies (barcodes evolution).
}
\label{fig:ph1}
\end{figure}

% and the network resulting from applying the customary threshold equal to the mean plus one standard deviation, $\epsilon=\mu +\sigma$.
% and across conditions, do they distinguish something
% The collection of the edges and the nodes gives us a binary network $B(X,\epsilon)$ for threshold $\epsilon$ which is the same as the network $G(V,E)$. Note that here the threshold is variable not fixed as in the orthodox approach based on graph theory a la Sporns.

\section{Conclusions and Discussion}
\label{se:conclusion}

Geometric questions have been pondered since the Greeks or even before, however, topological ones are only one century old  \cite{edelsbrunner2010computational}. A geometric space is defined via geometric measures (distances, areas, angles, etc.) and the relationships that are built on them. A topological space, on the other hand, defines a set of qualitative relationships between its parts: adjacency, containment, connectedness, overlap, cover, etc. \cite{aleksandrov1999mathematics}. Thus, A geometric space is defined in terms of quantitative features and the topological space uses
in terms of qualitative relationships.  Both set of features, the metrical and the topological are related but the relation is asymetrical, that is, topological (qualitative) features can be deduced from metrical (quantitative) ones but the inverse is not true. For example, knowing the location and length of two segments we know if they overlap, but knowing that one segment is adjacent to another one does not say nothing about where are they located or their length.

The most populat geometric model is the network model. Network modeling has became a standard de facto to study the spatial properties of biological systems. The wealth of studies linking network models with a range of phenomena as wide as genetic marker, genetic regulatory processes, viral evolution, neural coding, synaptic plasticity, cognition, neurodegenerative disorders and consciousness, speaks about the capacity of network modeling to represent objects and their relationships. 
%However, network models as any other modeling process rests upon its own assumptions and limitations.

Dynamic network data are easily available from multiple disciplines, not only the life sciences but specially from the social science, for example finances, social networks. A new approach for network comparison is persistent homology. The main advantage of this approach is that it allows to have a multiscale network framework avoiding the threshold determination problem. Persistent homology can be seen as the analysis of the connectivity pattern as the threshold varies. Thus, it avoids the ad-hoc specification of a threshold for the study of the overlapping network patterns and the rates at which they vary.


The basic concepts of persistent homology in relation to networks are
\begin{itemize}
\item network as a simplicial complex (addresses the dyadic limitation \ref{sse:dy})
\item multiscale network as graph filtration and dendogram (addresses the threshold selection problem \ref{sse:thr})
\end{itemize}

Topological neuroscience is an untapped resource for empirical neuroscientists.
Roughly speaking, the uses of topology in neuroscience can be categorized into three (overlapping) themes: (i) traditional topological data analysis applied to neuroscience; (ii) an upgrade to network science; and (iii) understanding the neural code \cite{curto2016can}.

\begin{enumerate}
\item  Calculate the statistics of the shape of point cloud data and more importantly the persistent homology (simplicial complex captures the information of intersecting balls, the simplicial complex can be Cech complex and Rips complex).
\item  "upgrade" of the network model: instead of a graph, one considers a simplicial complex. The higher-order simplices correspond to cliques (all-to-all connected subgraphs)
\item Suppose we are monitoring with painstaking detail the electrical firing at every time point of a neuron, what governs the behavior of this cell? the network, bien sure!  Now, imagine that we could monitor the activity of all the other neurons, and we knew exactly the pattern of connections between them, and were blessed with an excellent model describing all relevant dynamics, then (maybe?) we would be able to predict when our neuron will fire. This seems a far away project, but monitoring  single cells as we do now seemed impossible in the 50's. %lit
\end{enumerate}



\begin{acknowledgements}

\end{acknowledgements}

% BibTeX users please use
\bibliographystyle{spmpsci}
\bibliography{../bibliography-jgr/bibliojgr}   % name your BibTeX data base

\end{document}
% end of file template.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Appendix}
%YS
%Here we elaborate on the implications of application of persistent homology to resting state connectivity data seen in the previous section.


%NETWORK COMPARISON WITH FREQUENCY DOMAIN PERSISTENT HOMOLOGY. Ben Cassidy, Caroline Rae, Victor  Solo Thresholding (or selecting significance level) is problematic, particularly  %YS bar code built with honology  help in that, short bars are noise, long ones are persistent features.
% the approach of my eo ec paper:
%Thresholding :One method for working around this difficulty is filtrations, which record the results of every possible binarization of the network (simplicial complexes building by doing incremental threhold or ball size), along with the associated threshold values. With this approach we can use first order measure of structure in networks (or simplicial complexes) and  “second order” measures as functions of edge weight. Such functions carry information, for example, in their rate of change, where sudden phase transitions in network structure as one varies the threshold can indicate the presence of modules or rich clubs in networks. The area under such curves can help us characterize the geometric structure in the activity of (hippocampal) neural populations, without appealing to external stimuli or receptive field (\cite{giusti2015clique})
% www.ams.org/journals/bull/2009-46-02/S0273-0979-09-01249-X/ http://diglib.eg.org/handle/10.2312/SPBG.SPBG07.091-100
% $MapperGUI.py
%YS Ojo literal

% Krakauer
%With the advent of high-throughput genomics, transcriptomics, proteomics, and metabolomics, and functional imaging we have witnessed a technological revolution in biology have gone hand by hand with the rise of bioinformatics and ML prediciton.
%But there has been a lack of complementary conceptual theory that could help us organize the flood of facts. An emphasis on models, rather than theory.
%A persistent problem in biology is (contrary to statistichal mechanics) that regularities exist at aggregate levels of description, therefore an qualitatively different theory is required to explain these emergent phenomena than the theory describing the underlying microscopic dynamics (Anderson, 1972).
%In biology, unlike for traditional physical and chemical phenomena, many of the spatial and temporal scales interact. In physics, nuclear forces can be neglected when calculating plane- tary orbits as these are screened off over large distances. In biology, however, the lowest levels can have a direct impact on the highest levels (and vice versa), as in the case of genes that influence behavior and social structures and behavior that influence gene expression patterns.

%We discussed by means of examples points i and ii in the previous section. We pass to briefly  discuss here point iii or the neural coding. The idea is to give the reader the basic concepts  to understand what are this very novel approach hhas to say about a very old problem how the brain represents space.

%Hubel and Wiesel pioneering work in the visual cortex inserted a microelectrode in the primary visual cortex of an anesthesized cat, but they could not monitor, let alone control the activity of neighboring cells, they could just listen to one neuron at a time, what they could control was the stimulus (bars with certain angle). They found orientation-tuned neurons.

%O'Keefe made a remarkable discovery on the same lines, but more striking, because in the hippocampus there is not an obvious sensory path, but he reported that hippocampal place cells responded selectively to spatial locations.
%With this discovery and with the Grid cells later on mind, we may ask are neurons vertices in a network or autonomous device (sensors) attuned with the outside world? Both models can be valid but yield different conceptual and methodological consequences. The first view, neurons are nodes in a network, is endorsed by neural networks and the last by neural coding. In the neural network paradigm the neuron behavior is described via network properties and in neural coding theory the neuronal behavior is a black box with the focus in the relationship between the external stimulus and the neural activity.

%We know about the neural coding of neurons but not much abut the neural coding of networks, since it seems that they belong to two different conceptual schemes. But topology, a mathematical theory in which neuroscientists are not trained at all, may be the natural tool for understanding the neural code of brain networks.

%For place cells, when simultaneous recordings of place cells became possible, it was shown via statistical inference (using previously measured place fields) that the animal's position could indeed be inferred from population place cell activity. The place cell code, thus, naturally reflects the topology of the represented space.

% Curto review for neural coding (place cells)
%\cite{curto2016can} Some have argued that hippocampal place cell code is fundamentally topological in nature [12, 6] while others have argued that considerable geometric information is also present and can be extracted using topological methods [9, 18]. Dabaghian have worked towards disambiguate geometric and topological information: the place fields respected topological aspects of the environment more than metric features \cite{dabaghian2014reconceiving}. Also, about the grid cells, it turns out that the space represented by grid cells is not the full environment, but a torus.

%The wealth of studies linking network models with cognition, disease \cite{stefan2013epileptic}, \cite{stam2014modern} and novel biomarkers points out to a paradigm shift in systems, cognitive, and clinical neuroscience: namely, that brain function and alteration are inherently networked phenomena.
% Dialectics, so far focus on one side, the bivariate, next step is the poly variate, when we master that, interventional attack will make sense (causal)
% the road towards causation pass by mastering the polyadic structure of the system
% Dyadic/Polyadic, bivariate/multivariate.

% In large-scale neuroimaging, cognitive functions appear to be performed by a distributed set of brain regions and even at a smaller scale, the spatiotemporal patterns of interactions between a few neurons is thought to underlie basic information coding (Szatmary and Izhikevich 2010).%no entiendo izi paper

%Using the simple observation that place fields corresponding to nearby locations will overlap, the authors conclude that neurons corresponding to those fields will tend to be co-active. Importantly, because of their inherently non-pairwise nature, coactivation patterns of neurons or brain regions can be naturally encoded as simplicial complexes.
%Using the aptly (but coincidentally) named “Nerve Theorem” from algebraic topology, one can work backward from observed coactivity patterns to recover the intersection pattern of the receptive fields, describing a topological map of the animal’s environment.
%The topological approach serve to decode maps of the environment from observed cell activity.

%Dabaghian \cite{dabaghian2014reconceiving} shows that hippocampal place cells may ultimately be interested in a space's topological qualities (its connectivity) more than its geometry (distances and angles). After deforming an U track, the resulting place fields preserved the relative sequence of places visited along the track but did not vary with the metrical features of the track or the direction of the rat's movement.
%Dabaghian claims that the paper provides evidence that hippocampal networks represent spatial contiguities and not 2D Cartesian maps created by path integration. The experiment directly pitted against one another these two hypotheses by having rats run in a variable shaped track in the near-dark, and CA1 place cells tracked the linear contiguities much better than the 2D shapes created by various configurations.
%\section{Discussion}
%\label{se:discuss}
\subsection{Definitions}

\begin{definition}
Let M be a set, a topology O is a subset of the power set of M, $O \subseteq \mathcal{P}(M)$ iff O  satisfies three axioms:
\begin{itemize}
\item[i] $\emptyset \subseteq O, M \subseteq O$
\item[ii] $U \subseteq  O, V \subseteq O$, then, $U \cap V \subseteq O$
\item[iii] $U_i \subseteq O, \bigcup  U_i \subseteq O$
\end{itemize}
\label{def:topology}
\end{definition}

The first axiom states that the empty set and the set M are in the topology O, the second axiom that for any two subsets of O the intersection also exists in the topology O, and the third axiom that the union of any subset of O also belongs to O.

\begin{example}
%Example 1
Let $M = \{a,b,c\}$ and $O = \{\emptyset, \{a,b,c\}\}$. O is a topology since it holds the three axioms.
\label{ex:1}
\end{example}
%Example 2
\begin{example}
Let $M = \{a,b,c\}$ and $O = \{\emptyset, \{a\}, \{b\}\}$. O is not a topology since axiom ii does not hold true, $\{a\} \cup \{b\} \nsubseteq O$.
\label{ex:2}
\end{example}
%Example 3
\begin{example}
Let $M = \{a,b,c\}$ and $O = \{\emptyset, \{a\}, \{b\}, \{c\}, \{a,b\},\{b,c\},\{a,c\},\{a,b,c\}\}$. O is a topology, the three axioms are true.
\label{ex:3}
\end{example}
%Open set definition to Appendix
For any given set M, the topologies on the set M are bounded. Thus, the topology with the fewest number of elements is $O = \{\emptyset, M\}$ is called chaotic topology (Example \ref{ex:1}), and the topology with the maximum number of elements is $O = P(M)$ is called discrete topology (Example \ref{ex:3}).

\subsection{Haussdorf-Gra,ov metric space comparison}
% METRIC SPACE example Haussdorf
The Gramov-Hausdorff distance allows us to measure the distance between two metric spaces. In essence, it just finds the edge that is most different in the two networks. For example, for the node sets X and Y since both are projected to the same template, then the node $x_i \in X$ is simply map to node $y_i \in Y$. Formally, the Gramov-Hausdorff distance is
\begin{equation}
d_{}(X,Y) = \frac{1}{2}_{x_i \in X, y_j \in Y} max |d_X(x_i,x_j) - d_Y(y_i,y_j))|
\end{equation}
% Gromov distance in python http://stackoverflow.com/questions/30706079/hausdorff-distance-between-3d-grids
In figure \ref{fig:ph1} we provide an example of network comparison using resting state fMRI data.
First, we have a measurement set denoted as $X= \{x_1 ... x_p\}$ where $x_i$ refers to the time series associated with a region of interest (ROI) in the resting state fMRI data set (the preprocessing of the image is described in detailin the Appendix section).
The measurement $x_i$ can be assumed to be normally distributed $x_i ~ N(0,1)$, the distance between two measurements can be, for example, calculated with the Pearson correlation $c_{X}(x_i ,x_j)= 1 - corr(x_i,x_j)$ or other form of coupling in frequency domain for example phase coupling, power coupling etc.
% also Granger causality and other
%YS cor should be in abs value
The measurement set $X$ and the distance $c_X$ form the metric space $(X,c_X)$. The network constructed by thresholding the correlation matrix between nodes is as follows: there is a link between two nodes, if $c(x_i,x_j) \leq \epsilon$, otherwise the nodes are not connected.

Figure \ref{fig:ph1} (a) shows the pipeline -the correlation/covariance matrix we perform a filtration process with a vector $\epsilon$ of possible thresholds.
Figure \ref{fig:ph1} (b) displays the resulting networks of applying each of the possible thresholds contained in the threshold vector $\epsilon= \epsilon_0,... \epsilon_i, ..., \epsilon_n$. Figure \ref{fig:ph1} (c) shows the Gramov-Hausdorff distance between the networks.

% and the network resulting from applying the customary threshold equal to the mean plus one standard deviation, $\epsilon=\mu +\sigma$.
%and across conditions, do they distinguish something
% The collection of the edges and the nodes gives us a binary network $B(X,\epsilon)$ for threshold $\epsilon$ which is the same as the network $G(V,E)$. Note that here the threshold is variable not fixed as in the orthodox approach based on graph theory a la Sporns.



%YS Network as a SC , comparison.
%(Python Mapper software).
Figure \ref{fig:ph1} shows how by varying the threshold we obtain different networks, from totally disconnected to fully connected,  providing a distribution of networks in which distance can be studied, as for example with the Gromov Haussdorf distance. This distance is able to differentiate/or not between the two conditions.


\subsection{Clustering}

Cluster analysis or clustering is the mapping of a measurement set into subsets called clusters, so that observations in the same cluster are similar (points within each cluster are similar to each other and points from different clusters are dissimilar).
The notion of similarity can be defined quite liberally with the selection of a distance between any two data points. Thus, a measurement set $X$ and a distance $d$ form a metric space $(X,d)$.
A distance $d$ between two vectors $x,y$ in order to be such (metric) needs to fulfill the requirements: $d(x,y)$ is real non-negative and finite, is symmetric $d(x,y)=d(y,x)$, is unique $d(x,y)=0$ only if $x=y$ and holds the triangle inequality $d(x,z) \leq  d(x,y) + d(y,z)$.
There are many algorithms that construct clusterings on the metric space, for example k-means, fuzzy C-means, hierarchical clustering, spectral clustering etc.

Although cluster analysis has become a principle technique of knowledge discovery in a wide range of fields, including social networks, microarray gene expression \cite{nugent2010overview} and classification of brain disorders \cite{arbabshirani2017single} to cite a few, its implementation is not free of pitfalls and ambiguities \cite{carlsson2009topology}, \cite{ronan2016avoiding}.
For example, the arbitrariness in the choice of the threshold result in the lack of robustness in the data partition. Indeed, little is known
about consistency of most clustering algorithms \cite{von2008consistency}.
% functoriality

A clustering function f takes a metric space $(\mathcal{X},d)$ and returns a partition of the underlying set $\mathcal{X}$, $f:(\mathcal{X},d) \to \prod(\mathcal{X},d)$. Kleinberg's impossibility theorem for clustering demonstrates that there is no clustering function satisfying three simple properties: scale-invariance, richness and consistency \footnote{The Arrow's impossibility theorem is another example of an axiomatic approach,in this case in the social science. Arrow's impossibility theorem states that
 no rank-order voting system can be designed that always satisfies three fairness criteria \cite{arrow1950difficulty}}:
 %
\begin{enumerate}
\item[i] Scale-invariance: An ideal clustering function does not change its result when the data are scaled. For example, by changing the metric distance from miles to kilometers the clustering function $f$ produces same result of for any scale $ f(d) = f(\alpha d)$.
\item[ii] Richness: Any partition of a finite set of points $\mathcal{X}$, $\prod(\mathcal{X},d$ can be created for some metric $d$ on $\mathcal{X}$
\item[iii] Consistency:  If we stretch the data so that the distances between clusters increases and/or the distances within clusters decreases, then  the clustering function produces the same result. Formally, let $d$ and $d'$ be two distance function, then if for any pair of points $(i,j)$ in the same cluster,$d(i,f) \geq d'(i,j)$ and for every pair in different clusters $d(i,f) \leq d'(i,j)$ then clustering does not change, $f(d)= f(d')$
\end{enumerate}
% YS : Draw chart
%http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/
What this important result teaches us is that no clustering function that satisfies the axioms of scale-invariance, richness and consistency.
Why this theorem is so poorly known, we do not know. It certainly shed light in the limitation and drawbacks of clustering algorithms.
For example, in k-means, one of the most important clustering methods, the number of clusters is fixed to k, that is, there is no a "natural" number of clusters to be discovered, it is rather an assumption externally imposed by the modeler. Having the Kleinberg's theorem in mind, we can see that the question of how many clusters do the data set needs is ill-posed. It is similar to asking how many scales exist in the data. The answer is that unless there is  scale-invariance, structure emerges at different scales.
\subsection{The curse of dimensionality}
The curse of dimensionality refers to the situation in which the larger is the dataset the more sparse the data become. The rationale for this phenomenon is simple. For a 1-dimensional space, let us say that I need n random points to guarantee that they are separated a distance d. For a 2-dimensional space, I will need $n^2$ random points to guarantee a separation d, and for a 3 dimensional space $n^3$ and so on. Thus, the amount of data needed grows exponentially with the dimensionality. Technically speaking, this can be rephrased as local smoothing methods tend to perform poorly for multivariate data \cite{stone1982optimal},\cite{lavergne2008breaking}.
%YS put a chart?
\subsection{Causality}
%PROBABILITY, RANDOM VARIABLES, AND STOCHASTIC PROCESSES Papoulis, Athanasios Pillai, Unnikrishna
There is no conflict between causality and randomness or between determinism and probability if we agree, as we must, that scientific theories are not discoveries of the laws of nature but rather inventions of the human mind. Their consequences are presented in deterministic form if we examine the results of a single trial; they are presented as probabilistic statements if we are interested in averages of many trials. Thus, there is always uncertainty, also in the so called deterministic setting where the uncertainties are "with certain errors and in certain ranges of the relevant parameters" and in the stochastic "with a high degree of certainty if the number of trials is large enough." Classical mechanics holds within an error e provided that the neglected factors are smaller than $\varphi$.
%A biological relativity view of the relationships between genomes and phenotypes. (D. Noble)
There is no privileged scale of causality in biology to clarify the relationships between genomes and phenotypes.  The idea that genetic causes are primary views the genome as a program. Initially, that view was vindicated by the discovery of mutations and knockouts that have large and specific effects on the phenotype. But we now know that these form the minority of cases. Many changes at the genome level are buffered by robust networks of interactions in cells, tissues and organs.  The 'differential' view of genetics therefore fails because it is too restrictive. An 'integral' view, using reverse engineering from systems biological models to quantify contributions to function, can solve this problem. % downward causation

GCM (J.Pearl) Causality is inferences under change. A causal model is a symbolic system that encodes the invariance, what remains constant.
Conditioning probabilities do not work for intervention, if they did, curing symptoms would cure diseases
%https://youtu.be/RPgvfSeQB8A
Dichotomy between causal and statistical notions.

Statistical notions or notions that can be defined with a joint distribution are Regression, Controlling for (conditioning), odd and risk ratios, Granger causality (Collapsability), Propensity score (Rubin's propensity score matching, make the groups receiving treatment and not-treatment more comparable).
Causal notions are spurious correlations (volume conduction pb), randomnization (you cant tell from the observation, we need additional assumptions, we need an intervention), confounding, explanatory variables.
A confound, a lurking variable or a confounder is a variable in a statistical model that correlates (directly or inversely) with both the dependent variable and an independent variable,  in a way that "explains away" some or all of the correlation between these two variables (if z affects to x and y, and x and y are not correlated it may look that x and y are correlated (z has explained away xy , confounding). note that confounding is a causal notion while correlation is not.

%http://voxeu.org/article/limitations-randomised-controlled-trials
Cartwhright, no causes in no causes out, in order to have causal conclusions you need to have causal assumptions. Example of statistical assumptions, normal distribution, independence ... but we need the judgemental causal assumptions because only those encode the invariance.

The important point is that causal assumptions cannot be expressed in the language of standard statistics. In 1920 Sewell Wright (and the early econometricians) created the SEM.
%http://ftp.cs.ucla.edu/pub/stat_ser/r370.pdf  The Causal Foundations of Structural Equation Modeling
The paper starts with "The role of causality in SEM research is widely perceived to be, on the one hand, of pivotal methodological importance and, on the other hand, confusing, enigmatic, and controversial.", and the same can be said about causality. The origins are more ilustrious, Aristotle, Hume etc but the contrast between the insidious and necessary and the enigmatic is there.

Causal effects in observational studies can only be substantiated from a combination of data and untested theoretical assumptions, not from the data alone.
%Many SEM textbooks have subsequently considered the term “causal modeling” to be an outdated misnomer (e.g., Kelloway, 1998, p. 8), giving clear preference to causality-free nomenclature such as “covariance structure,” “regression analysis,” or “si- multaneous equations.”
%Comparing structural equation models to the potential-outcome framework, Sobel (2008) asserts that “in general (even in randomized studies), the structural and causal parameters are not equal, implying that the structural parameters should not be interpreted as effect.”
Rubin and Sobel are from the potential-outcome framework. also Paul Holland which states that the equation $y = a +bx+ \epsilon$ what really means is a way to expresses the $p(y|x)$. But for Pearl, the  structural interpretation of this equation has nothing to do with the conditional distribution of ${y}$ given  ${x}$ ; rather, it conveys causal information that is orthogonal to the statistical properties of  ${x}$  and  ${y}$. For Pearl, the SEM language in its nonparametric form offers a mathematically equivalent alternative to the potential-outcome framework that Holland and Sobel advocate for causal inference. This explains why SEM retains its status as the prime language for causal and counterfactual analysis.

\cite{wiedermann2016statistics}
Not to confuse X(correlation) with Y(causation) falls a little flat, specially since we dont know what causation means, correlation on the other hand we know many, maybe too many ways to calculate ad plot it in Matlab,R, SPSS or any of the plethora of statistical modeling software available these days.
The counterfactual or “interventionist” account lead by Woodward and Pearl is gaining momentum.

%Statistics and Causality: Methods for Applied Empirical Research. Chapter 11
In cell biology causality networks are also called gene regulatory networks. In neuroscience, causality networks are widely used to express the temporal interactions between various regions of the brain.
%Andrew Lo physics envy https://arxiv.org/pdf/1003.2688.pdf
\subsubsection{Approaches for the reconstruction of the causal relations}
Hypergraphs, can record any possible collection of relations but it is precisely this degree of generality leads to a combinatorial explosion in systems of modest size.
Giusti et al, argues that simplicial complexes gives a compact and computable encoding of relations between arbitrarily large subgroups while retaining access to a host of quantitative tools for detecting and analyzing the structure of the systems they encode. In particular, the homology \footnote{Names of topological objects have a seemingly pathological tendency to conflict with terms in biology, so long have the two subjects been separated. Mathematical homology has no a priori relationship to the usual biological notion of homology} of a simplicial complex is a collection of topological features called cycles that one can extract from the complex.

In 1969 Granger introduced a method to quantify temporal-causal relations among time series measurements. He introduced Wiener’s concept of causality into the analysis of time series (Wiener,1956)
%Wiener, N. (1956) The theory of prediction, in Modern Mathematics for Engineers
Granger causality (LR model) is impaired by several crucial problems of discovering latent confounding effect, missing counterfactual reasoning and capturing instantaneous and nonlinear causal relationships (Spirtes et al., 2001, Pearl, 2009, Bahadori and Liu, 2013b).
Granger is not causal model rather it is a temporal dependence discovery method.
GC characterize the extent to which a process influences another process and builds upon the notion of incremental predictability. Thus $x_t$ Granger causes another process $y_t$ if the future values of y can be better predicted with past values of x and y than with past values of y alone.
%Bunge book Causality
The historical antecedent of the philosophy behind Grange's causality is the father of modern economics, Paul Samuelson [34] who adopted the regular-succession-in-time view of causation  and, as a consequence, regards difference equations and differential equations with time as the independent variable, as causal laws.
However, there need to be nothing causal about a regular sequence of events and, unless the equations describing it are enriched with semantic assumptions pointing to causal factors, the equations are neither causal nor noncausal.


The bivariate model is:
$y_t = a_0 \sum_{k=1}^{L}b_{1k}y_{t-k} \sum_{k=1}^{L}b_{2k}x_{t-k} + \phi_t$
where $\phi_t$ are uncorrelated random  variables of mean 0 and variance $\sigma^2$, L is the time lag and tells us the number of past values taken into consideration. The null hypothesis that $x_t$ does not Granger cause $y_t$, that is,$b_{2k} = 0$ for $k=1..L$.

It can be straightforwardly extended to p-dimensional multivariate time series $x_t \in R^{p \times l }$ (p number of variables, for example, space locations (roi) l is the length of the time series).
Hume cause must precede the effect.
The approximation problem
$x_t \sim \sum_{p=1}^{P} \sum_{l=1}^{L}\beta_{l}^{p} x_{t-l}^{p}$
The approximation problem can be approximated with least squares to estimate the coefficients $\beta_{l}^{p}$ from a system of linear equations.


Granger causality on gene regulatory networks with a large p may not lead to satisfactory results (Lozano et al., 2009). The statistical significant tests are inefficient, while they lead to higher chances of spurious correlations. Moreover, the high dimensionality of biological data leads to further challenges (sparse parameter vector). To address this issue, various variable selection procedures can be applied (Lasso, LARS, elastic nets).
Lasso is an alternative regularized version of least squares, which, in addition to the minimization of the residual sum of squares, imposes an norm on the coefficients.LARS (least angle regression) is a less greedy version of traditional forward selection method is computationally less intensive compared to Lasso makes it widely used in variable selection problems. However, for highly correlated variables, Lasso and LARS tend to select only one variable instead of the whole group. The the elastic net method addresses this problem outperformed Lasso in terms of prediction error for correlated data.

The problem of the reconstruction of a gene regulatory network belongs to the class of inverse problems with high-dimensional data set and sparse number of measurements. A general inverse problem can be seen as an operator equation $y = A \beta$ where y is the effect, $\beta$ is the cause and A is the model between the cause and its effect. The approximation problem can be seen as an inverse problem.
In practice, one has to take into account that the data y  are noisy. The data y deviate from the ideal data $y*$ and the norm $||y - y* ||$ is the noise.
Inverse problems are often ill-posed, which that the above Equation (using the noisy data y) may have no solution, or more than one Regularization methods are proposed to deal with the ill-posedness of inverse problem (e.g. Tikhonov)

% from Bunge Causality, notes 3rd edition
As for final causation, laughed away by Rabelais, Bacon, and Spinoza, it is now back in biology under the name of ‘teleonomy’ (cf. Mayr [19], Monod [17], Ayala [20]). The simplest difference of bacteria and a nonliving thing is that the former is goal-directed, or has a ‘teleonomic project’ built into it. downward causation: term ‘causation’ is being misused in this context, for what is at stake is a multilevel system.
The truth seems to lie in a synthesis of the upward and the downward views, neither of these should be formulated in terms of causation because levels, being sets, cannot act upon one another.
This is precisely why we need a mathematical language other than sets (networks), for example, complexes, categories etc. because once you pick up your threshold you have fixed the  set, and sets cannot act upon each other.
%YS how can i translate "sets dont act upon eachother" in mathematical formalism?

% http://www.nature.com/nature/journal/v541/n7636/full/541156a.html Poldrack review of "Sex, Lies, and Brain Scans: How fMRI Reveals What Really Goes on in our Minds"
fmri reveals only correlations. Just because an area is active when one experiences fear, it doesn't mean that that region is necessarily involved in the experience of fear. Causal necessity can be demonstrated only by manipulating the function of a brain region, either through brain stimulation or the study of brain lesions. For instance, many studies show that the ventromedial prefrontal cortex (important in value-based decision-making) is activated when research participants are considering how much they are willing to pay for consumer goods. But recent work has found that some people with lesions to this area show no impairment in such abilities (A. R. Vaidya and L. K. Fellows Nature Commun. 6, 10120; 2015).
%future works
Signal processing in graphs: The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs.
\cite{shuman2013emerging}
In the previous section we defined simplicial complex and persistent homology.
We approach this from the point of view of graph theory, that is, introducing simplicial complex as a generalization of networks, introducing the filtration process used to overcome the fixed threshold problem.

Persistent Homology provides the methodology for the qualitative analysis of the relations between the simplices generated in the filtration process. It is possible to extend this analysis from a topological perspective, without relying on the network as a geometric object. In this section we sketch topological data analysis and its relevance in analyzing qualitative features of on point data cloud. The emphasis is now in assessing the shape of data, that is, we are agnostic about the spatial arrangement of data, note that this is different from the previous section in which the data were network objects.

\begin{definition}
The elements of the topology O are called open sets. Formally, if the subset U, $U \subseteq M$ and U is in the topology O, $U \subsete O$, then U is called an open set.
The notion of an open set allows us to predicate on the nearness of points in a topological space without having to impose any specific distance between the data points, for example, an euclidean distance.
Connectivity and continuity can be studied using the open set notion. For example, two points are topologically close if the are in the same neighborhood and the neighborhood of a point is an open set that contains that point.
\end{definition}
\subsubsection*{Time series}
The computation of distances between nodes when they represent time series needs special consideration. Time-series networks where each node has a record of scalar time series is pervasive in brain functioning modeling, for example electrophysiology (EEG) and functional magnetic resonance imaging (fMRI).
Metric distance between nodes is unsuited for networks of autocorrelated time-series.
% YS: why?because they are autocorrelated?

The most basic distance between two nodes or recorded time series is $d(x,y) = \sqrt{1 - \rho_{x,y}}$, where $\rho_{x,y}=\frac{cov(x,y)}{var(x)var(y)}$.  From a signal processing point of view, we can calculate the distance as the closeness or how well can we predict one with the other, eg. predicting $y_t$ from $x_t$ (regression problem). The prediction error is $e_{y|x} = y_t - \alpha x_t$, where alpha $\frac{cov(x,y)}{var(x)}$.
The next step is to capture autocorrelation using pointwise frequency coherence measures.

For Cassidy, Rae and Solo there are two type of distances, marginal and joint distances. Marginal distances, for example, the Mahalanobis distance is based on marginal distribution statistics such as means, covariances and densities. For example, a marginal distance between two nodes of recorded time series $(x,y)$ can be calculated as, $d(x,y) = \sqrt{1 - \rho_{x,y}}$, where $\rho_{x,y}=\frac{cov(x,y)}{var(x)var(y)}$.
From a signal processing point of view, we can calculate the distance as the closeness or how well can we predict one with the other, e.g. predicting $y$ from $x$ (regression problem). The prediction error is $e_{y|x} = y - \alpha x$, where alpha $\frac{cov(x,y)}{var(x)}$. For network comparison, marginal distances are inappropriate because the data are collected jointly. To understand this point we need to have present that marginal probability may be thought of as an unconditional probability.
Marginal distance are unaffected by the threshold choice we can't explain away the other variables as marginal probability assumes.
%YS: put example

Joint distances, on the other hand, are based on joint statistics like cross spectra, mutual information and correlation. Pearl makes an important distinction between statistical and causal notions. Regression, Controlling for (conditioning), odd and risk ratios, Granger causality (Collapsability), Propensity score (Rubin's propensity score matching, make the groups receiving treatment and not-treatment more comparable) are all statistical notions based on joint distributions.
Causal notions are spurious correlations (volume conduction), randomization, confounding factors, explanatory variables.

\section{Definitions Appendix}

Mathematics is, however, protean and new languages that deal with the curse of dimensionality problem and the network threshold selection problem are available. 
For example, the graph-theoretic formalism can't tell the difference between three or more nodes just "conected" or acting as a pacemaker.
For example, a large threshold will exclude weak connections, loosing potentially relevant information.

%%%%% Lecture on Topology %https://www.youtube.com/watch?v=7G4SqIboeig Begin
%at the coarsest level spaceset is a set, that is, it consists on a set of points,but this is not enough, something needs to ber added if we want to talk about for example continuity. We need to add structure on set to be able to work with continuity. The weakkes structure that can be establish in a set that allows a good definition of ciontinuity of maps, this minimal structure is topology. (motivation for using topology)
Definition -- Power set
The power set of a M is the set containing all subsets of M, including the empty set and M itself. For example, the power set of $M={1,2}$ is $ \mathcal{P}(M) = \{\emptyset, \{1\},\{2\},\{1,2\}\}$.

The axioms that define topological space can be given in terms of open sets or using the concept of neighborhood. Here we define topological space using open sets, for the neighborhood definition see \cite{brown2006topology}.

Definition -- homeomorphism
An homeomorphism is a continuous map that establishes a one-to-one correspondence between two objects. Two objects are homeomorphic if they can be deformed into each other by a continuous, invertible mapping.
A  map $f:M \to N$ is continuous iff the preimage of any open set is open.

The preimage or invserve image of a set V, $V \exists N$ defined in the map $f:M \to N$ is the set of elements in the domain M that land in V.
Example Draw figure

Definition -- Continuous map:
To build a map we just need two sets, but to answer the question is the map continuous, we need topology. Whether a map, $f: M \to N$  is continuous depends on which topologies are chosen on both the domain set M and the target set N.

Definition: Let the topological spaces,  $(M,O_m)$ and $(N,O_N)$ and the map, $f:M \to N$ is called continuous with respect to the topology of $O_M$ and $O_N$ if for every set that is open in the target, $\forall \exists O_N$, the preimage is an open set in the domain, $preim_f(V) \exists O_M$ (the preimage(V) is the set of elements in the domain that land in V)

%min 52
Example: Check whether the map is continuous with respect tot he chosen topologies, we find the primages of the target
The composition of continuous maps is also continuous

Definition -- Convex Hull
Convex hull or convex envelope is the smallest convex set that contains all points in set X. Computing the convex hull means constructing an unambiguous, efficient representation of the required convex shape. The convex hull of the given points is identical to the set of all their convex combinations.
A convex combination is a linear combination of points (which can be vectors, scalars, or more generally points in an affine space) where all coefficients are non-negative and sum to 1. Formally give n points in real vector space, $x_1, ... x_n$ a convex combination is a point $y=a_{1}x_1 + ... + a_{n}x_n$ such that the sum of the weights is 1, $\sum a_i = 1$. We can also by the same token obtain the convex combination of probability distributions.

For example, every convex combination of two points lies on the line segment between the points, then in an edge we have the convex hull or the 2-1(=1)-simplex. For three points, every convex combination will lie in the area, so the triangle is a convex hull or a 3-1(=2)-complex. %YS dont see this, why every linear combination must lie in the straight line or in the triangle for 3points?
% https://en.wikipedia.org/wiki/Convex_combination
%mean corrected (Scanner drift is corrected by detrending and mean correction is the simplest way to detrend data -normalizing to the mean signal intensity of each run other more complicated forms of detreanding is wavelets, polinomial interpolation etc.)



%Examples of simplicial complexes%%%%

%Poincaré argues that what science can attain is not the things themselves,
%as the naive dogmatists think, but only the relations between the things. Outside of these relations there is no knowable reality.
%Since the interaction with the world is described by information, it is by dealing with information that these systems most effectively persist.

%Researchers in big data applied to biomedicine are always trying to come to terms with the huge data flow available. However, the pressure for having better diagnosis and personalized treatment will be too strong to ignore. This customer's pressure will translate from clinicians to researchers, already is.
Examples of simplicial complexes applied to brain data are the clique complex, the concurrence complex \cite{ellis2012describing}; \cite{curto2008cell}; Dowker 1952), its Dowker dual (Dowker 1952), and the independence complex (Kozlov 2007).
Of course, a graph is a simplicial complex. Graphs are good for encoding dyadic relationships, Clique Complex can do polyadic canonical extension of graphs, Concurrence/Dual complex is good at capturing relationships between two variables of interest eg: time and activity or activity in two distant regions and  Independence

Clique complex:  a clique is all-to-all connected subgraph. Given a graph,replace every vertice replaces every clique by a simplex on the vertices participating in the clique. This procedure produces a clique complex that has been used for neural analysis of hippocapal cells in both spatial and non spatial behavior \cite{giusti2015clique} and in general for correlation and coherence maps of for example fMRI data.
%This application demonstrates that simplicial complexes are sensitive to organizational principles that are hidden to graph statistics, and can be used to infer parsimonious rules for information encoding in neural systems.
Thus, Clique complexes precisely encode the topological features present in a graph, so clique complexes "complete" graphs providing new views on the topological properties of graphs.
However, other types of simplicial complexes can be used to represent information that cannot be so encoded in a graph.

There are other complexes, like concurrent complexes (for coactivation but I dont see how they give more than coherence matrices) or Independence complex (hypergraph) to study system’s community structure (Bassett).
%Topological Data analysis (TDA) provides geometric summaries of our data. Data has shape and shape has meaning.
%Topology is about shape, and shape is the global realization of local constraints. %YS this is problematic because if topology is about shape why it is hard to get the shape from the topological properties. This motto cant referred to shape as geometric shape . all boils down to how you define the relationships, if a metric (euclidean) or some sort of similarity, in that last case one couldnt possibly reconstruct the shape

%https://youtu.be/gtFVdGb9Y8w?t=5m30s
%Given a metric on the rows, and the columns are the features and we can work our shape with the metric, but note that we can relax the study of shape and use similarity rather than metric.
%Once you have a metric + a threshold of statistical relevance the data has a (topological) shape, they occupy parts in the metric space, there is gaps, holes, flairs, bubbles...and with this we can stop and ask what the shape we just devised tells us about the data.
%Since traditional graph-theoretical methods may not be sufficient to understand the immense complexity of such a biological network, we explored whether methods from algebraic topology could provide a new perspective on its structural and functional organization.
%An extension of this ideas is directedness of information flow has been used to investigate the relationship between simulated structural and functional neural networks \cite{dotko2016topological}. % Markram




"Marginal" distance because they used to be found by summing values in a table along rows or columns, and writing the sum in the margins of the table.
%http://www.statisticshowto.com/marginal-distribution/  https://en.wikipedia.org/wiki/Marginal_distribution
%time series distance http://stats.stackexchange.com/questions/185912/alternate-distance-metrics-for-two-time-series

Joint distances on the other hand, are based on joint statistics like cross spectra, mutual information and correlation. If the data are collected separately in time and space then there is no point in calculating joint distance, rather we need marginal distance. On the other hand, for network comparison, marginal distances are inappropriate because the data are collected jointly.
% YS: wy not, i have women and men and i want to know how they like fish or meat.
% I can day in day one and site 1 plus day 2 site 2 and calculate the marignal, p(someone like meat) p(somelike fish).
% I think this refers exclusively for time series
% for network analysis we cant do just marginal , because there p(a) is unconditional to any other event, that is, we must assume that the nodes (time series recirded at different (brain) location are not coupled)

%A confound, a lurking variable or a confounder is a variable in a statistical model that correlates (directly or inversely) with both the dependent variable and an independent variable,  in a way that "explains away" some or all of the correlation between these two variables (if z affects to x and y, and x and y are not correlated it may look that x and y are correlated (z has explained away xy , confounding). note that confounding is a causal notion while correlation is not.

%pearl knows that joint, neither marginal nor conditional are non causal concept, so he created a new language for causal networks, but always networks, always dyadic, the problem of coactivity will be there, also the pb of the threshold

%Once a choice of the topology is made, directly given by the open set notion, it becomes possible to study the properties of continuity, connectedness, and compactness of the data set. Three points close to each other are said to be coactive (more on this in section). Each choice of open sets for a space is called a topology.
%A more useful topology is the standard topology which is defined as follows. For a set $M = \mathbb{R}^d = \mathbb{R} \times \mathbb{R} \times \mathbb{R} ... \mathbb{R}$, $O \exists \mathcal{P}(\mathbb{R}^d)$. The soft ball for any number $r \exists \mathbb{R}^+$ and point $p \exists  \mathbb{R}^d$, $B_r(p) = \{q_1...q_d\}| \sum_{i=1}^{d}(q_i - p_i)^2 < r^2$.
%Example  min 24 %https://www.youtube.com/watch?v=7G4SqIboeig

%AT branch of math concerned with properties of geometric objects that are invariant under continuous deformations (bending, twisting, stretching, but not tearing). A dought and a coffe mug is the same but an eight an a donught are "unrelated" i can not make one the other, because an eight has 2 holes. Thus, the key property invariant under continuous deformation is the presence of holes. So, AT is largely concerned with formalizing the notion of a hole in a geometric object. i-dimensional holes.
%0-D holes are connected components of geometric objects, 1-D hole in 3-D objects are holes you can see through (cheese holes) a 2-D in a 3-D object is a hollow space, for instance a ballon, you cant see through but there is a hole, the ballon traps air inside
%AT provides a mechanism for counting holes, which is Betti numbers. For a dimensional object $\mathcal{X}$, we define $B_i($\mathcal{X}$)$, the ith Betti number of $\mathcal{X}$ to be the number of i-dimensional holes in $\mathcal{X}$.
%For example a double donught (eight), $B_0(X) = 1$, one connected component, $B_1(X)=2$, hole I can see through and $B_2(X)=0$, no hollow spaces.
%For discretely represented geometric objects, $B_i(X)$, is easily computable via linear algebra.
%For every space X and integer k, get a boolean vector space $H_k(X)$, dimension of H_0 is the number of connected components, and dimension of $H_k$ counts the number of k-dimensional holes., the dimension oif K_k is called Betti number. A continuous map f induces linear transformation (fucntoriality)
%Two's company, three (or more) is a simplex
%Just as with network models, once we have effectively encoded the data of interest in a simplicial complex, it is necessary to find useful quantitative measurements of the resulting structure to draw conclusions about the system from which the data were collected.
%We start by generalizing familiar graph statistics to be used in simplicial complexes. The simplest local measure of structure – the degree of a vertex – naturally becomes a vector-measurement whose entries are the number of maximal simplices of each size in which the vertex participates, this vector is more intuitively thought of as a generalization of the clustering coefficient of the vertices. Other local and global statistics such as efficiency and path length can be generalized by considering paths through simplices of some fixed size, which provides a notion of robust connectivity between vertices of the system (Dlotko et al. 2016).
%In addition to these more or less straight extensions of quantitative graph measure into simplicial complexes, however the power of algebraic topology comes with the use of algebraic topology for example, homology of the complex to identify homology cycles.

With optogenetics, DBS, TMS etc. one can stimulate single neurons or specific groups of neurons to control their function. To meet this need, one can construct a different type of filtration \cite{taylor2015topological} that construct a sequence of simplicial complexes with a time parameter, labeling each simplex as “on” or “off” at each time, and require that once simplices “turn on” they remain so indefinitely. This type of filtration is of use in  diffusion model of fronto-temporal dementia and in contagion models (Taylor et al. 2015), where a simplex becomes active once sufficiently many nearby simplices are active.

A homeomorphism between two topological spaces X,Y is a bijective function such that $f:X \to Y$ and $f^{-1}$ are both continuous. If topological spaces are homeomorphic then they are topologically equivalent, that is, have the same topology type.

The basic question in topology is to classify spaces up to topology type.
Note that the problem of classification is ill-defined it depends on what we have in mind we want and how to classify, classification or clustering is subjective science, despite these problems we still need to classify because this is the only way we can understand reality. %min 7. https://vimeo.com/105644331


%Algebraic Topology --- Holes qualitative features of data--(problems) -- PH
% Metric data set (high dimensional space) -- Build geometric filtered complex on top of data -- compute persistent homology of the complex
